{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#242e87 solid; padding: 15px; background-color: #9feced; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color='#242e87'>ðŸ’¡ Inspiration:</font></h3>\n    \n* Purpose: to predict Base Price for Insurance\n    \n* We are going to use two separate models in order to predict \n    * Frequency of insurance claim\n    * Severity of insurance claim\n    <br>    <br>\n* Then, we are going to multiply those in order to find potential cost for each person","metadata":{}},{"cell_type":"markdown","source":"**Main plan is trying to create a base model for each frequency and severity first and then improve after comparing the predictions for base price**\n","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split \nimport matplotlib.pyplot as plt\nfrom category_encoders import TargetEncoder\nimport xgboost as xgb\nfrom xgboost import plot_importance\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-03T22:14:57.225417Z","iopub.execute_input":"2023-07-03T22:14:57.225749Z","iopub.status.idle":"2023-07-03T22:14:59.386513Z","shell.execute_reply.started":"2023-07-03T22:14:57.225725Z","shell.execute_reply":"2023-07-03T22:14:59.384538Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/car-insurance-claim/file(3).csv\")\ndf = pd.DataFrame(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:14:59.388873Z","iopub.execute_input":"2023-07-03T22:14:59.389175Z","iopub.status.idle":"2023-07-03T22:14:59.504254Z","shell.execute_reply.started":"2023-07-03T22:14:59.389148Z","shell.execute_reply":"2023-07-03T22:14:59.503142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The column descriptions can be seen here\nurl = \"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2566208%2F50dd4f899017ad7c6ba848cfa513a853%2FScreenshot_2020-04-17%20GreyAtom%20-%20Learning%20Platform(1).png?generation=1587141048165615&alt=media\"\nresponse = requests.get(url)\nimg = Image.open(BytesIO(response.content))\nimg","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:14:59.5057Z","iopub.execute_input":"2023-07-03T22:14:59.506226Z","iopub.status.idle":"2023-07-03T22:14:59.788097Z","shell.execute_reply.started":"2023-07-03T22:14:59.506189Z","shell.execute_reply":"2023-07-03T22:14:59.786589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:14:59.790461Z","iopub.execute_input":"2023-07-03T22:14:59.791217Z","iopub.status.idle":"2023-07-03T22:14:59.830971Z","shell.execute_reply.started":"2023-07-03T22:14:59.791191Z","shell.execute_reply":"2023-07-03T22:14:59.830322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are some missing values\n# But I am not going to handle those yet\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:14:59.832105Z","iopub.execute_input":"2023-07-03T22:14:59.832569Z","iopub.status.idle":"2023-07-03T22:14:59.858219Z","shell.execute_reply.started":"2023-07-03T22:14:59.832545Z","shell.execute_reply":"2023-07-03T22:14:59.857594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's handle the weird columns first\ndf = df.applymap(lambda x: x.replace('$', '') if isinstance(x, str) else x)\ndf = df.replace(to_replace=r'^z_', value='', regex=True)\ndf = df.applymap(lambda x: x.replace(',', '') if isinstance(x, str) else x)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:14:59.859427Z","iopub.execute_input":"2023-07-03T22:14:59.859871Z","iopub.status.idle":"2023-07-03T22:15:00.265439Z","shell.execute_reply.started":"2023-07-03T22:14:59.859849Z","shell.execute_reply":"2023-07-03T22:15:00.264436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turn the columns into floats if available after the cleaning\nfor i in df.columns:\n    if df[i].dtypes == \"object\":\n        try:\n            df[i] = df[i].astype(float)\n        except:\n            pass","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.266673Z","iopub.execute_input":"2023-07-03T22:15:00.267031Z","iopub.status.idle":"2023-07-03T22:15:00.288359Z","shell.execute_reply.started":"2023-07-03T22:15:00.267005Z","shell.execute_reply":"2023-07-03T22:15:00.28639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.290534Z","iopub.execute_input":"2023-07-03T22:15:00.290993Z","iopub.status.idle":"2023-07-03T22:15:00.323242Z","shell.execute_reply.started":"2023-07-03T22:15:00.290939Z","shell.execute_reply":"2023-07-03T22:15:00.32144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating real severity and frequence columns\ndf_freq_sev = df.groupby('ID').agg({'CLAIM_FLAG': 'mean', 'CLM_AMT': 'mean'})\ndf_freq_sev = df_freq_sev.reset_index()\ndf_freq_sev.columns  = ['ID', 'FREQUENCY', 'SEVERITY']\ndf_freq_sev","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.324763Z","iopub.execute_input":"2023-07-03T22:15:00.325114Z","iopub.status.idle":"2023-07-03T22:15:00.349042Z","shell.execute_reply.started":"2023-07-03T22:15:00.325084Z","shell.execute_reply":"2023-07-03T22:15:00.347957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since we are going to use the aggregated versions of those we are dropping the columns\ndf_merged = df.drop(columns = ['CLAIM_FLAG', 'CLM_AMT'])\n\n# Then merge with the df_freq_sev\ndf_final = pd.merge(left = df_freq_sev, right=df_merged, how = 'left', on = 'ID')\ndf_final","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.353108Z","iopub.execute_input":"2023-07-03T22:15:00.353658Z","iopub.status.idle":"2023-07-03T22:15:00.408455Z","shell.execute_reply.started":"2023-07-03T22:15:00.353635Z","shell.execute_reply":"2023-07-03T22:15:00.407134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping duplicate entries\ndf_final.drop_duplicates(inplace= True)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.409575Z","iopub.execute_input":"2023-07-03T22:15:00.409965Z","iopub.status.idle":"2023-07-03T22:15:00.437883Z","shell.execute_reply.started":"2023-07-03T22:15:00.409926Z","shell.execute_reply":"2023-07-03T22:15:00.43698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Test split \nBefore anything I do, to avoid data leakage I am splitting the data","metadata":{}},{"cell_type":"code","source":"train_data, test_data = train_test_split(df_final, test_size=0.2, \n                                         random_state=42, stratify=df_final.FREQUENCY)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.438928Z","iopub.execute_input":"2023-07-03T22:15:00.439467Z","iopub.status.idle":"2023-07-03T22:15:00.456793Z","shell.execute_reply.started":"2023-07-03T22:15:00.439439Z","shell.execute_reply":"2023-07-03T22:15:00.455812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some Utility Functions","metadata":{}},{"cell_type":"code","source":"def plot_preds(y_true, y_preds):\n    plt.hist(y_preds, bins = 100, alpha = 0.5, label = 'Predictions')\n    plt.hist(y_true, bins = 100, alpha = 0.5, label = 'Real')\n    \n    plt.xlabel('Sample')\n    plt.ylabel('Value')\n    plt.title('Prediction vs Real')\n    \n    plt.legend()\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.457734Z","iopub.execute_input":"2023-07-03T22:15:00.459574Z","iopub.status.idle":"2023-07-03T22:15:00.466423Z","shell.execute_reply.started":"2023-07-03T22:15:00.459513Z","shell.execute_reply":"2023-07-03T22:15:00.465531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_xgb_model(params):\n    return xgb.XGBRegressor(**params)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.467711Z","iopub.execute_input":"2023-07-03T22:15:00.468416Z","iopub.status.idle":"2023-07-03T22:15:00.487734Z","shell.execute_reply.started":"2023-07-03T22:15:00.468388Z","shell.execute_reply":"2023-07-03T22:15:00.486062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Frequency Prediction\nWe are going to use a modified metric and XGBoost Regressor as model","metadata":{}},{"cell_type":"code","source":"# We want to model this using Poisson since we want to find the occurence count for each value\ndf_final['FREQUENCY'].hist()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.489401Z","iopub.execute_input":"2023-07-03T22:15:00.49004Z","iopub.status.idle":"2023-07-03T22:15:00.781962Z","shell.execute_reply.started":"2023-07-03T22:15:00.49001Z","shell.execute_reply":"2023-07-03T22:15:00.780977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_for_freq(train_data, test_data):\n    \n    X_train = train_data.drop(['FREQUENCY', 'ID', \"SEVERITY\"], axis=1)\n    y_train = train_data['FREQUENCY']\n    \n    X_test = test_data.drop(['FREQUENCY', 'ID', \"SEVERITY\"], axis=1)\n    y_test = test_data['FREQUENCY']\n    \n    # Again to avoid data leakage I am fitting the encoder to only train and encoding the test using train fitted encoder\n    encoder = TargetEncoder()\n    X_train_encoded = encoder.fit_transform(X_train, y_train)\n    X_test_encoded = encoder.transform(X_test, y_test)\n\n    return X_train_encoded, X_test_encoded, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.782993Z","iopub.execute_input":"2023-07-03T22:15:00.784341Z","iopub.status.idle":"2023-07-03T22:15:00.790697Z","shell.execute_reply.started":"2023-07-03T22:15:00.784296Z","shell.execute_reply":"2023-07-03T22:15:00.789223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_freq(model, train_data = train_data, test_data = test_data):\n    \n    # Process the data\n    X_train, X_test, y_train, y_test = preprocess_for_freq(train_data, test_data)\n\n    # Fit the model\n    model_fit = model.fit(X_train, y_train)\n\n    y_pred = model_fit.predict(X_test)\n\n    # We want to fit the means \n    metric = (1 - y_test.mean() / y_pred.mean())\n\n    print('Final metric: %f' % (metric))\n\n    return model_fit, y_pred","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.791835Z","iopub.execute_input":"2023-07-03T22:15:00.792101Z","iopub.status.idle":"2023-07-03T22:15:00.804625Z","shell.execute_reply.started":"2023-07-03T22:15:00.792054Z","shell.execute_reply":"2023-07-03T22:15:00.803781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_freq_params = {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 250}\nxgb_freq_model = create_xgb_model(xgb_freq_params)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.805417Z","iopub.execute_input":"2023-07-03T22:15:00.805706Z","iopub.status.idle":"2023-07-03T22:15:00.822543Z","shell.execute_reply.started":"2023-07-03T22:15:00.805684Z","shell.execute_reply":"2023-07-03T22:15:00.821709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_freq, preds_freq = train_test_freq(xgb_freq_model, train_data, test_data)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:00.823531Z","iopub.execute_input":"2023-07-03T22:15:00.824306Z","iopub.status.idle":"2023-07-03T22:15:03.597089Z","shell.execute_reply.started":"2023-07-03T22:15:00.824255Z","shell.execute_reply":"2023-07-03T22:15:03.596342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Severity Prediction\nWe are going to use a modified metric and XGBoost Regressor as model","metadata":{}},{"cell_type":"code","source":"# We are going to remove 0 values in order to model Severity\ndf_final['SEVERITY'].hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:03.598725Z","iopub.execute_input":"2023-07-03T22:15:03.599078Z","iopub.status.idle":"2023-07-03T22:15:03.940272Z","shell.execute_reply.started":"2023-07-03T22:15:03.599049Z","shell.execute_reply":"2023-07-03T22:15:03.939324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This looks like a Gamma distribution and we are going to use Gamma as objective in our XGBRegressor model\ndf_final[df_final['SEVERITY'] != 0]['SEVERITY'].hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:03.94138Z","iopub.execute_input":"2023-07-03T22:15:03.941683Z","iopub.status.idle":"2023-07-03T22:15:04.286373Z","shell.execute_reply.started":"2023-07-03T22:15:03.941659Z","shell.execute_reply":"2023-07-03T22:15:04.284674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_for_sev(train_data, test_data):\n    # Make the train data non-zero to make the Gamma function work \n    train_nonzero = train_data[train_data['SEVERITY'] != 0]    \n    \n    X_train = train_nonzero.drop(['FREQUENCY', 'ID', \"SEVERITY\"], axis=1)\n    y_train = train_nonzero['SEVERITY']\n    \n    X_test = test_data.drop(['FREQUENCY', 'ID', \"SEVERITY\"], axis=1)\n    y_test = test_data['SEVERITY']\n    \n    encoder = TargetEncoder()\n    X_train_encoded = encoder.fit_transform(X_train, y_train)\n    X_test_encoded = encoder.transform(X_test, y_test)\n    \n    return X_train_encoded, X_test_encoded, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:04.288225Z","iopub.execute_input":"2023-07-03T22:15:04.288557Z","iopub.status.idle":"2023-07-03T22:15:04.29419Z","shell.execute_reply.started":"2023-07-03T22:15:04.288534Z","shell.execute_reply":"2023-07-03T22:15:04.293307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_sev_params = {'objective': 'reg:gamma', 'learning_rate': 0.1,\n          'max_depth': 5, 'alpha': 10, 'n_estimators': 250}\nxgb_sev_model = create_xgb_model(xgb_sev_params)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:04.295482Z","iopub.execute_input":"2023-07-03T22:15:04.296213Z","iopub.status.idle":"2023-07-03T22:15:04.310306Z","shell.execute_reply.started":"2023-07-03T22:15:04.296142Z","shell.execute_reply":"2023-07-03T22:15:04.309118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_sev(model, train_data = train_data, test_data = test_data):\n    # Process the data\n    X_train, X_test, y_train, y_test = preprocess_for_sev(train_data, test_data)\n\n    # Fit the model\n    model_fit = model.fit(X_train, y_train)\n\n    y_pred = model_fit.predict(X_test)\n\n    # Because we have a Gamma-like distribution we want to fit the median\n    metric = (1- np.median(y_test) / np.median(y_pred))\n\n    print('Final metric: %f' %  metric)\n\n    return model_fit, y_pred","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:04.311791Z","iopub.execute_input":"2023-07-03T22:15:04.312279Z","iopub.status.idle":"2023-07-03T22:15:04.325816Z","shell.execute_reply.started":"2023-07-03T22:15:04.312228Z","shell.execute_reply":"2023-07-03T22:15:04.324598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot looks like it can be improved but let's check and compare on Base Price\nmodel_sev, preds_sev = train_test_sev(xgb_sev_model, train_data, test_data)\nplot_preds(test_data[test_data['SEVERITY'] != 0]['SEVERITY'], preds_sev)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:04.32695Z","iopub.execute_input":"2023-07-03T22:15:04.327215Z","iopub.status.idle":"2023-07-03T22:15:05.593907Z","shell.execute_reply.started":"2023-07-03T22:15:04.327194Z","shell.execute_reply":"2023-07-03T22:15:05.592518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Calculations","metadata":{}},{"cell_type":"code","source":"# Let's merge the predictions for severity and frequency\npreds_freq_series = pd.Series(preds_freq, index=test_data.index, name = 'FREQ_PREDS')\ndf_merged_final = pd.concat([test_data, preds_freq_series], axis = 1)\n\npreds_sev_series = pd.Series(preds_sev, index=test_data.index, name = 'SEV_PREDS')\ndf_merged_final = pd.concat([df_merged_final, preds_sev_series], axis = 1)\ndf_merged_final.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:05.595202Z","iopub.execute_input":"2023-07-03T22:15:05.595533Z","iopub.status.idle":"2023-07-03T22:15:05.624486Z","shell.execute_reply.started":"2023-07-03T22:15:05.595505Z","shell.execute_reply":"2023-07-03T22:15:05.623376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Base Price columns based on freq and sev\ndf_merged_final['BASE_PRICE_PREDS'] = df_merged_final['FREQ_PREDS'] * df_merged_final['SEV_PREDS']\ndf_merged_final['BASE_PRICE_REAL'] = df_merged_final['FREQUENCY'] * df_merged_final['SEVERITY']\ndf_merged_final.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:05.625642Z","iopub.execute_input":"2023-07-03T22:15:05.62592Z","iopub.status.idle":"2023-07-03T22:15:05.652124Z","shell.execute_reply.started":"2023-07-03T22:15:05.625896Z","shell.execute_reply":"2023-07-03T22:15:05.650893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looks like we are in debt for 750k for now\npercent_dif = df_merged_final['BASE_PRICE_PREDS'].sum() / df_merged_final['BASE_PRICE_REAL'].sum()\nreal_dif = df_merged_final['BASE_PRICE_PREDS'].sum() - df_merged_final['BASE_PRICE_REAL'].sum()\nreal_price = df_merged_final['BASE_PRICE_REAL'].sum()\nprint(f'Real Base Price: {real_price}\\n\\\nPercentage difference between real and predictions: {percent_dif}\\n\\\nReal difference between real and predictions: {real_dif} ')","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:05.657488Z","iopub.execute_input":"2023-07-03T22:15:05.657828Z","iopub.status.idle":"2023-07-03T22:15:05.665733Z","shell.execute_reply.started":"2023-07-03T22:15:05.657802Z","shell.execute_reply":"2023-07-03T22:15:05.664208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We are trying to cover the large losses with distributing the cost of those to more profitable people \n# It looks promising yet it can be improved still\nplt.hist(df_merged_final['BASE_PRICE_PREDS'],alpha = 0.5, bins = 100, label = 'Pred')\nplt.hist(df_merged_final[(df_merged_final['BASE_PRICE_REAL']< 20000) & (df_merged_final['BASE_PRICE_REAL'] !=0) ]['BASE_PRICE_REAL'],\n         alpha = 0.5, bins = 100,label = 'Real')\n\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:05.667631Z","iopub.execute_input":"2023-07-03T22:15:05.667963Z","iopub.status.idle":"2023-07-03T22:15:06.160957Z","shell.execute_reply.started":"2023-07-03T22:15:05.667935Z","shell.execute_reply":"2023-07-03T22:15:06.159723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimizing the model","metadata":{}},{"cell_type":"code","source":"# We are going to model the large losses and add the large loss effect to our predictions in order to push our predictions to right\ntrain_clipped = train_data[train_data['SEVERITY'] < 7500]","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:06.1621Z","iopub.execute_input":"2023-07-03T22:15:06.162399Z","iopub.status.idle":"2023-07-03T22:15:06.169562Z","shell.execute_reply.started":"2023-07-03T22:15:06.162377Z","shell.execute_reply":"2023-07-03T22:15:06.168252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_sev_model_clipped = create_xgb_model(xgb_sev_params)\nmodel_sev_clipped, preds_sev_clipped = train_test_sev(xgb_sev_model_clipped, train_clipped, test_data)\nplot_preds(test_data[test_data['SEVERITY'] != 0]['SEVERITY'], preds_sev)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:06.171054Z","iopub.execute_input":"2023-07-03T22:15:06.171492Z","iopub.status.idle":"2023-07-03T22:15:07.134521Z","shell.execute_reply.started":"2023-07-03T22:15:06.171456Z","shell.execute_reply":"2023-07-03T22:15:07.132757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_freq_model_clipped = create_xgb_model(xgb_freq_params)\nmodel_freq_clipped, preds_freq_clipped = train_test_freq(xgb_freq_model_clipped, train_clipped, test_data)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:07.135516Z","iopub.execute_input":"2023-07-03T22:15:07.135798Z","iopub.status.idle":"2023-07-03T22:15:08.901507Z","shell.execute_reply.started":"2023-07-03T22:15:07.135774Z","shell.execute_reply":"2023-07-03T22:15:08.900845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_freq_clipped_series = pd.Series(preds_freq_clipped,index = test_data.index,name = 'FREQ_CLIPPED_PREDS')\ndf_clipped_final = pd.concat([test_data, preds_freq_clipped_series], axis = 1)\npreds_sev_clipped_series = pd.Series(preds_sev_clipped,index = test_data.index,name = 'SEV_CLIPPED_PREDS')\ndf_clipped_final = pd.concat([df_clipped_final, preds_sev_clipped_series], axis = 1)\ndf_clipped_final","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:08.90262Z","iopub.execute_input":"2023-07-03T22:15:08.903074Z","iopub.status.idle":"2023-07-03T22:15:08.934402Z","shell.execute_reply.started":"2023-07-03T22:15:08.903052Z","shell.execute_reply":"2023-07-03T22:15:08.933709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clipped_final['BASE_PRICE_PREDS'] = df_clipped_final['FREQ_CLIPPED_PREDS'] * df_clipped_final['SEV_CLIPPED_PREDS']\ndf_clipped_final['BASE_PRICE_REAL'] = df_clipped_final['FREQUENCY'] * df_clipped_final['SEVERITY']\ndf_clipped_final.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:08.935534Z","iopub.execute_input":"2023-07-03T22:15:08.935795Z","iopub.status.idle":"2023-07-03T22:15:08.960999Z","shell.execute_reply.started":"2023-07-03T22:15:08.935772Z","shell.execute_reply":"2023-07-03T22:15:08.95975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clipped_final[\"LARGE_LOSS_EFFECT\"] = len(df_clipped_final[\"BASE_PRICE_REAL\"]) / len(test_data) * df_clipped_final.BASE_PRICE_REAL.mean()\ndf_clipped_final.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:08.962149Z","iopub.execute_input":"2023-07-03T22:15:08.962484Z","iopub.status.idle":"2023-07-03T22:15:08.992817Z","shell.execute_reply.started":"2023-07-03T22:15:08.962459Z","shell.execute_reply":"2023-07-03T22:15:08.991517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clipped_final['BASE_PRICE_PREDS_w_LL'] = df_clipped_final['BASE_PRICE_PREDS'] + df_clipped_final['LARGE_LOSS_EFFECT']\ndf_clipped_final.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:08.994526Z","iopub.execute_input":"2023-07-03T22:15:08.995732Z","iopub.status.idle":"2023-07-03T22:15:09.019445Z","shell.execute_reply.started":"2023-07-03T22:15:08.995684Z","shell.execute_reply":"2023-07-03T22:15:09.017988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_preds(df_clipped_final[df_clipped_final['BASE_PRICE_REAL']!= 0]['BASE_PRICE_REAL'], df_clipped_final['BASE_PRICE_PREDS_w_LL'])","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:09.020704Z","iopub.execute_input":"2023-07-03T22:15:09.021098Z","iopub.status.idle":"2023-07-03T22:15:09.550182Z","shell.execute_reply.started":"2023-07-03T22:15:09.021074Z","shell.execute_reply":"2023-07-03T22:15:09.549023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looks like we are in debt for 750k for now\npercent_dif = df_clipped_final['BASE_PRICE_PREDS_w_LL'].sum() / df_clipped_final['BASE_PRICE_REAL'].sum()\nreal_dif = df_clipped_final['BASE_PRICE_PREDS_w_LL'].sum() - df_clipped_final['BASE_PRICE_REAL'].sum()\nreal_price = df_clipped_final['BASE_PRICE_REAL'].sum()\nprint(f'Real Base Price: {real_price}\\n\\\nPercentage difference between real and predictions: {percent_dif}\\n\\\nReal difference between real and predictions: {real_dif} ')","metadata":{"execution":{"iopub.status.busy":"2023-07-03T22:15:09.552598Z","iopub.execute_input":"2023-07-03T22:15:09.554296Z","iopub.status.idle":"2023-07-03T22:15:09.564692Z","shell.execute_reply.started":"2023-07-03T22:15:09.554238Z","shell.execute_reply":"2023-07-03T22:15:09.563113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the future work, some columns may be grouped or further processed to create more meaningful features.","metadata":{}}]}